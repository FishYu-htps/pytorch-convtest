import torch
import time


#os.environ["MIOPEN_ENABLE_LOGGING_CMD"] = "1"
#os.environ["MIOPEN_LOG_LEVEL"] = "5"
#os.environ["MIOPEN_ENABLE_LOGGING_ELAPSED_TIME"] = "1"
#os.environ["MIOPEN_DEBUG_CONV_GEMM"] = "0"

# Convolution settings
batch_size = 32
height = 256
width = 256
in_channels = [64,8]
out_channels = [64,32]
stride = (1, 1)
padding = [1,3]
kernel = [3,7]

# Number of warmup and measured runs
warmup = 2
runs = 10

# Dtypes to test
dtypes = {
    "float32": torch.float32,
    "float16": torch.float16,
}

def benchmark(dtype_name, dtype, kernel,i):
    print(f"shape:[{in_channels[i]},{out_channels[i]},{kernel}]")
    print(f"\nTesting {dtype_name}...")
    kernel_size = (kernel, kernel) if isinstance(kernel, int) else kernel
    # 直接創建 NCHW / OIHW
    x = torch.randn((batch_size, in_channels[i], height, width), dtype=dtype, device="cuda")  # NCHW
    w = torch.randn((out_channels[i], in_channels[i], *kernel_size), dtype=dtype, device="cuda") # OIHW

    conv = torch.nn.functional.conv2d

    # Warmup
    for _ in range(warmup):
        y = conv(x, w, stride=stride, padding=padding[i])
        torch.cuda.synchronize()

    # Timed run
    start = time.time()
    for _ in range(runs):
        y = conv(x, w, stride=stride, padding=padding[i])
        torch.cuda.synchronize()
    end = time.time()
    #print(f"last result:{y}")
    total_time = end - start
    avg_time = total_time / runs

    # Approx FLOPs for Conv2D: 2 * H * W * Cout * (Cin * Kx * Ky) * B
    flops = 2 * height * width * out_channels[i] * (in_channels[i] * kernel_size[0] * kernel_size[1]) * batch_size
    tflops = flops / avg_time / 1e12

    print(f"Avg time: {avg_time:.6f} s, Throughput: {tflops:.2f} TFLOP/s")

i=0
for kernel_size in kernel:
    print("\n===========================================\n")
    for name, dtype in dtypes.items():
        try:
            benchmark(name, dtype,kernel_size,i)
        except Exception as e:
            print(f"Skipped {name} due to error: {e}")
    i=i+1

